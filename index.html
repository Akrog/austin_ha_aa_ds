<!DOCTYPE html>
<html>
  <head>
    <title>Cinder High Availability Active-Active Design Session</title>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
    <link rel="stylesheet" href="slides.css"></link>
  </head>
  <body>
    <textarea id="source">

layout: true
name: title_layout
class: left, rh_title_slide, no_page_footer

---

layout: true
name: section_layout
class: left, rh_section_slide, no_page_footer

---

layout: true
name: default_layout
class: left

---

template: title_layout

# Cinder High Availability Active-Active Design Session

### OpenStack Summit Austin

.small[
<br>April 27th 2016]

---

# Design Session Documents

Blueprint: .small[https://blueprints.launchpad.net/cinder/+spec/cinder-volume-active-active-support]

<br>
Austin:

- Slides: .small[http://akrog.github.io/austin_ha_aa_ds]
- Etherpad: .small[https://etherpad.openstack.org/p/cinder-newton-activeactiveha]

<br>
Tokyo:

- Slides: .small[http://akrog.github.io/cinder/20151029%20-%20tokyo_ha_aa_ds.pdf]
- Etherpad: .small[https://etherpad.openstack.org/p/mitaka-cinder-cvol-aa]

---

# Status Overview:

- [Support HA Active/Active:](https://review.openstack.org/232599) Specs&#10003; · Code: 80% done (25% merged) 

- [Remove Cinder API races:](https://review.openstack.org/207101) Specs&#10003; · Code: 80% done (50% merged)

- [Add Tooz locks:](https://review.openstack.org/202615) Specs&#10003; · Code: 90% done (80% merged)

- [Job Distribution:](https://review.openstack.org/232595) Specs&#10067; · Code: 100% done (0% merged)

- [Resource cleanup:](https://review.openstack.org/236977) Specs&#10067; · Code: 100% done (0% merged)

- [Auto-fencing:](https://review.openstack.org/237076) Specs&#10007; · Code: 0% done<br>
  Different approach needed, going back to the drawing board

- Optional: [Remove Manager Local Locks:](https://review.openstack.org/237602) Specs&#10067; · 0% done

*<br>.red[NOTE: We are also talking about Backup Nodes]*

---
template: section_layout
class: center, middle, white, no_page_footer

**.big[API Races]**

---

# API Races

Didn't diverge from approach agreed in Tokyo:

- Compare-and-swap was used
- Generic conditional update mechanism added to OVOs
    * Base functionality merged
    * Merged patches using base functionality
    * Some patches required changes to SQLAlchemy
        - SQLAlchemy has been updated
        - Now patches can be merged
- API returns Generic errors (no loops)

.mildgreen[Status: Patches need reviewing and at least 1 needs to be redone]

---
template: section_layout
class: center, middle, white, no_page_footer

**.big[Job Distribution]**

---

# Mechanism

.center.medium[![](assets/job_distribution.svg)]

---

# Implementation

- Cluster grouping is done by host (imagine it as a reverse proxy)
- Add *node_name* configuration option
- Add *service_nodes* table
    * Move fields from service table:
        + *rpc_version*
        + *object_version*
        + *report_count*
        + *heartbeat*
    * Moves require 2 releases (rolling upgrades)
        + In N &#10142; Compatibility code aggregates data from both tables
        + In O &#10142; Remove compatibility code and fields from service
- New mechanism for determining when service is up
- Disabling still at service level &#10142; Signals to drain node
---

# Implementation (II)

- New API microversion for Service list:
    * Returns service ID
    * Returns service nodes summary with
        + Number of nodes under the host
        + Number of nodes that are down
        + Last heartbeat from the nodes
    * Optionally can return service nodes summary list
- New "*service_nodes*" API endpoint:
    * Summary list
    * Detailed list
    * Show

<br>
.mildgreen[Status: Server and Client patches up for review]

---

# Diverge from Tokyo

- Approach proposed in Tokyo:
    * Add cluster configuration option
    * Use cluster as message topic key
    * Change everywhere "*host*" with "*cluster*"

--

- Reasons for the change:
    * Complexity:
        + Rename of "*host*" with "*cluster*" &#10142; 12K LOC - High probability of introducing bugs
        + Multiple releases needed
        + Compatibility code for all APIs
    * Inconsistency:
        + In documentation
        + In existing APIs &#10142; Say host, but is actually cluster
        + Between configuration and APIs
    * Clean backports impossible with previous release

---
template: section_layout
class: center, middle, white, no_page_footer

**.big[Resource Cleanup]**

---

# Mechanism

- Problem: Know which resources must be cleaned:
    * Host in resource is not enough
    * By the node on startup
    * By other node from the same host if node doesn't come up

--

- Solution:
    * For each node track resources of cleanable operations
    * Use the DB
    * Nodes cleanup on start
    * We can do cleanup of down node
        + Manually
        + Automatically

---

# Implementation

- Cleans same resources in the same way as we do now
- Introduce concept of cleanable resources (OVOs)
    * Resources: Volumes, Snapshots, Backups
    * Not all operations require cleanup &#10142; Defined on OVO depends on:
        + Object version
        + Current status
- New "*worker*" table to track in flight cleanable resources
    - On RPC call entry is created
    - On reception by c-vol and c-bak
        * Update DB entry with worker node
        * Create new entry if not called from the API
    - Removing entries from DB on
        * Exit manage method if resource status doesn't match worker
        * Reset status API

---

# Implementation (II)

- Cleanup mechanism:
    * c-vol & c-back nodes on restart
    * From the scheduler
        * Manual mechanism
            + API &#10142; Scheduler &#10142; c-vol/c-bak
            + Filters available to delimit resource cleanup (by binary, host, node, resource, UUID...)
        * Automatic mechanism
            + Periodic checks of down nodes &#10142; Cleanup after T down time
            + Informs other Scheduler of cleanups &#10142; Prevent multiple cleanup requests
            + Disabled by default &#10142; Triggered by API
        * APIs return nodes being cleaned and skipped (no node to do cleanup)
        * Timestamp request &#10142; Prevent race conditions
        * During upgrades
            + Cleanup API is disabled &#10142; Receiving node may not know how to clean it
            + Nodes still clean their own work

.mildgreen[Status: Server and Client patches up for review]

---

# Diverge from Tokyo

- Approach proposed in Tokyo:
    * All main manager methods create DB entries
    * Schedulers don't coordinate and all send cleanup to nodes
    * Cleanups request are on a resource basis
    * On node startup cleanups are distributed between all nodes in the host

--

- Reasons for changes:
    * Complexity
        + Checking resource status and which node set it in all DB entries
        + Doing it in a non racy manner
    * Performance killer
        + Checking for cleanups
        + One entry for each main method call
        + Cleanups on a resource basis

---

# Issues with Current implementation

.left-column.big[![](assets/cleanup_issues_1.svg)]
.right-column[
.medium[
- Cases:
    * No node
        + Message doesn't reach broker
        + Broker dies and loses request
    * No worker entry
- Solutions
    * Manual:
        + Implement worker list with filters
        + Change status manually
    * Add to cleanup mechanism
        + Timeout config option
        + Option A: Cleanup workers without node
        + Option B:
            - Don't add entry on RPC request
            - Clean cleanable state and no worker
]
]

---

# Issues with Current implementation (II)

.left-column.big[![](assets/cleanup_issues_2.svg)]
.right-column[
.medium[
- Cleaning node dies during cleanup
- Auto cleanup won't clean it again
- Solutions
    * On node clean request reception:
        + Change node in entries to current node
        + Check worker entries before changing
        + Complex query or one for each table
    * Add mechanism
        + Scheduler marks cleanup as requested
        + Nodes report cleanup reception
        + Scheduler marks cleanup as initiated
        + Nodes report completion
        + Scheduler marks cleanup as initiated
        + When node down, retry cleanup jobs
]
]

---
template: section_layout
class: center, middle, white, no_page_footer

**.big[Auto fencing]**

---

# Disruption Scenarios

- Message broker lost &#10142; No big deal
    * We cannot receive new jobs
    * We cannot respond to sync requests
- DB connection lost
    * No heartbeats &#10142; Scheduler will consider it as down
    * Cannot update status or remove cleanup work
    * Will proceed with cleanups &#10142; Only if auto cleanup is enabled
- DLM lost
    * Locks will be freed
    * Multiple nodes will access resource
    * Defeats the purpose of having locks

---

# Options

- Requires more study
- Stop all threads and don't accept RPC requests
    * Non trivial
    * Margin for recovery before cleanup
        + On DB loss and then recovery
            - Without auto cleanup &#10142; OK
            - With auto cleanup &#10142; OK if time < cleanup time
        + On locks it would depend on the DLM &#10142; Not nice
    * On disruption resolution &#10142; Restart mechanism

- Ignore the issue
    * Locks
        + Deletion of volume that is being used for creation
            - Worst case corruption?
            - If we stop creating volume is useless anyway

---
template: section_layout
class: center, middle, white, no_page_footer

**.big[HA A-A without a DLM]**

---

# No DLM for manager locks

- Only for the manager locks
- For when drivers don't require a DLM
- Use workers table for locking
    * New cleanup resource Type: *Lock*
    * Store the resource ID
    * As status store the operation
- Alternative &#10142; Configure Tooz to use DB locks

---

template: section_layout

# <center>Should try to merge early</center>

.medium[
- Make sure no regressions are introduced to Active-Passive
- Validate Active-Active
]

## <center>Patches ready for review</center>

.left-column.tiny[
API Races:

.min_line_height[
- [Add ordering possibilities to conditional update](https://review.openstack.org/231936)
- [Remove API races on extend and volume_upload_image](https://review.openstack.org/216378)
- [Remove API races from migrate and retype](https://review.openstack.org/221442)
- [Removed potential races from volume update method](https://review.openstack.org/277588)
- [Remove API races from consistency groups](https://review.openstack.org/259429)
]

Job Distribution

.min_line_height[
- [Refactor sqlalchemy service methods](https://review.openstack.org/286598)
- [Add service_nodes table and related methods](https://review.openstack.org/286599)
- [Add ServiceNode Versioned Object](https://review.openstack.org/286600)
- [Change Job Distribution for HA A-A](https://review.openstack.org/286601)
- [Add new API job distribution features](https://review.openstack.org/294263)
- [Cinderclient: Allow getting node summary info for services](https://review.openstack.org/304234)
- [Cinderclient: Add service node commands](https://review.openstack.org/304235)
]
]

.right-column.tiny[
Cleanup

.min_line_height[
- [Add workers table](https://review.openstack.org/303018)
- [Add worker's DB operations](https://review.openstack.org/303019)
- [Add cleanable base object and cleanup request VO](https://review.openstack.org/303020)
- [Make c-vol use workers table for cleanup](https://review.openstack.org/303021)
- [Allow triggering cleanup from API](https://review.openstack.org/303022)
- [Make c-bak use workers table for cleanup](https://review.openstack.org/303023)
- [Add auto cleanup mechanism](https://review.openstack.org/303024)
- [Cinderclient: Add service node cleanup command](https://review.openstack.org/304236)
- [Cinderclient: Add Backup to cleanable resource types](https://review.openstack.org/304237)
- [Cinderclient: Add service node auto-cleanup](https://review.openstack.org/304238)
]

Tooz Locks

.min_line_height[
- [Start/Stop coordinator with Services](https://review.openstack.org/263313)
]

]

---

template: section_layout
background-image: url("assets/cc.svg")
class: center, middle, white, no_page_footer

<br><br><br>
Except where otherwise noted, this work is licensed under

http://creativecommons.org/licenses/by/4.0/

    </textarea>
    <script src="remark.js" type="text/javascript"></script>
    <script type="text/javascript">
      var slideshow = remark.create({
        ratio: '16:9',
        countIncrementalSlides: false
      });
    </script>
  </body>
</html>

<!-- vim: set ft=markdown : -->
