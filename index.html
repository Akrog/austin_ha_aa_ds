<!DOCTYPE html>
<html>
  <head>
    <title>Cinder High Availability Active-Active Design Session</title>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
    <link rel="stylesheet" href="slides.css"></link>
  </head>
  <body>
    <textarea id="source">

layout: true
name: title_layout
class: left, rh_title_slide, no_page_footer

---

layout: true
name: section_layout
class: left, rh_section_slide, no_page_footer

---

layout: true
name: default_layout
class: left

---

template: title_layout

# Cinder High Availability Active-Active Design Session

### OpenStack Summit Austin

.small[
<br>April 27th 2016]

---

# Design Session Documents

Blueprint: .small[https://blueprints.launchpad.net/cinder/+spec/cinder-volume-active-active-support]

<br>
Austin:

- Slides: .small[http://akrog.github.io/austin_ha_aa_ds]
- Etherpad: .small[https://etherpad.openstack.org/p/cinder-newton-activeactiveha]

<br>
Tokyo:

- Slides: .small[http://akrog.github.io/cinder/20151029%20-%20tokyo_ha_aa_ds.pdf]
- Etherpad: .small[https://etherpad.openstack.org/p/mitaka-cinder-cvol-aa]

---

# Status Overview:

- [Support HA Active/Active:](https://review.openstack.org/232599) Specs&#10003; · Code: 80% done (25% merged) 

- [Remove Cinder API races:](https://review.openstack.org/207101) Specs&#10003; · Code: 80% done (50% merged)

- [Add Tooz locks:](https://review.openstack.org/202615) Specs&#10003; · Code: 90% done (80% merged)

- [Job Distribution:](https://review.openstack.org/232595) Specs&#10067; · Code: 100% done (0% merged)

- [Resource cleanup:](https://review.openstack.org/236977) Specs&#10067; · Code: 100% done (0% merged)

- [Auto-fencing:](https://review.openstack.org/237076) Specs&#10007; · Code: 0% done<br>
  Different approach needed, going back to the drawing board

- Optional: [Remove Manager Local Locks:](https://review.openstack.org/237602) Specs&#10067; · 0% done

*<br>.red[NOTE: We are also talking about Backup Nodes]*

---

# API Races

Didn't diverge from approach agreed in Tokyo:

- Compare-and-swap was used
- Generic conditional update mechanism added to OVOs
    * Base functionality merged
    * Merged patches using base functionality
    * Some patches required changes to SQLAlchemy
        - SQLAlchemy has been updated
        - Now patches can be merged
- API returns Generic errors (no loops)

.mildgreen[Status: Patches need reviewing and at least 1 needs to be redone]

---

# Job Distribution - Mechanism

.center.medium[![](assets/job_distribution.svg)]

---

# Job Distribution (II) - Implementation

- Cluster grouping is done by host (imagine it as a reverse proxy)
- Add *node_name* configuration option
- Add *service_nodes* table
    * Move fields from service table:
        + *rpc_version*
        + *object_version*
        + *report_count*
        + *heartbeat*
    * Moves require 2 releases (rolling upgrades)
        + In N &#10142; Compatibility code aggregates data from both tables
        + In O &#10142; Remove compatibility code and fields from service
- New mechanism for determining when service is up
- Disabling still at service level &#10142; Signals to drain node
---

# Job Distribution (III) - Implementation (II)

- New API microversion for Service list:
    * Returns service ID
    * Returns service nodes summary with
        + Number of nodes under the host
        + Number of nodes that are down
        + Last heartbeat from the nodes
    * Optionally can return service nodes summary list
- New "*service_nodes*" API endpoint:
    * Summary list
    * Detailed list
    * Show

<br>
.mildgreen[Status: Server and Client patches up for review]

---

# Job Distribution (IV) - Diverge from Tokyo

- Approach proposed in Tokyo:
    * Add cluster configuration option
    * Use cluster as message topic key
    * Change everywhere "*host*" with "*cluster*"

--

- Reasons for the change:
    * Complexity:
        + Rename of "*host*" with "*cluster*" &#10142; 12K LOC - High probability of introducing bugs
        + Multiple releases needed
        + Compatibility code for all APIs
    * Inconsistency:
        + In documentation
        + In existing APIs &#10142; Say host, but is actually cluster
        + Between configuration and APIs
    * Clean backports impossible with previous release

---

# Resource Cleanup - Mechanism

- Problem: Know which resources must be cleaned:
    * Host in resource is not enough
    * By the node on startup
    * By other node from the same host if node doesn't come up

--

- Solution:
    * For each node track resources of cleanable operations
    * Use the DB
    * Nodes cleanup on start
    * We can do cleanup of down node
        + Manually
        + Automatically

---

# Resource Cleanup (II) - Implementation

- Cleans same resources in the same way as we do now
- Introduce concept of cleanable resources (OVOs)
    * Resources: Volumes, Snapshots, Backups
    * Not all operations require cleanup &#10142; Object says if is cleanable:
        + For specific object version
        + For specific status
- New "*worker*" table to track in flight cleanable resources
- On RPC call entry is created
- On reception by c-vol and c-bak
    * Update DB entry with worker node
    * Create new entry if not called from the API
- Reset status API removes entry

---

# Resource Cleanup (III) - Implementation (II)

- Cleanup mechanism:
    * c-vol & c-back nodes on restart
    * From the scheduler
        * Manual mechanism
            + API &#10142; Scheduler &#10142; c-vol/c-bak
            + Filters available to delimit resource cleanup (by binary, host, node, resource, UUID...)
        * Automatic mechanism
            + Periodic checks of down nodes &#10142; Cleanup after T down time
            + Informs other Scheduler of cleanups &#10142; Prevent multiple cleanup requests
            + Disabled by default &#10142; Triggered by API
        * APIs return nodes being cleaned and skipped (no node to do cleanup)
        * Timestamp request &#10142; Prevent race conditions
        * During upgrades
            + Cleanup API is disabled &#10142; Receiving node may not know how to clean it
            + Nodes still clean their own work

.mildgreen[Status: Server and Client patches up for review]

---

# Resource Cleanup (IV) - Diverge from Tokyo

- Approach proposed in Tokyo:
    * All main manager methods create DB entries
    * Schedulers don't coordinate and all send cleanup to nodes
    * Cleanups request are on a resource basis
    * On node startup cleanups are distributed between all nodes in the host

--

- Reasons for changes:
    * Complexity
        + Checking resource status and which node set it in all DB entries
        + Doing it in a non racy manner
    * Performance killer
        + Checking for cleanups
        + One entry for each main method call
        + Cleanups on a resource basis

---

template: section_layout

# <center>PATCHES READY FOR REVIEW</center>

---

template: section_layout
background-image: url("assets/cc.svg")
class: center, middle, white, no_page_footer

<br><br><br>
Except where otherwise noted, this work is licensed under

http://creativecommons.org/licenses/by/4.0/

    </textarea>
    <script src="remark.js" type="text/javascript"></script>
    <script type="text/javascript">
      var slideshow = remark.create({
        ratio: '16:9',
        countIncrementalSlides: false
      });
    </script>
  </body>
</html>

<!-- vim: set ft=markdown : -->
